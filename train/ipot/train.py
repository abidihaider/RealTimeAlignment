"""
Train an detector calibration model on ROM data with IPOT
"""

import os
import argparse
from pathlib import Path
from pprint import pprint
import yaml
import pandas as pd

# == torch ======================================
import torch
from torch import nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import MultiStepLR

# == sparse poi =================================
from rtal.datasets.dataset import ROMDataset
from rtal.utils import (get_lr,
                        Checkpointer,
                        Cumulator,
                        get_data_root)
from rtal.models.ipot import InducedPointFeaturizer


os.environ['OMP_NUM_THREADS'] = '24'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'


def run_epoch(model,
              loss_fn,
              dataloader, *,
              device,
              optimizer = None,
              desc      = None):
    """
    run one epoch on a data loader
    """

    cumulator = Cumulator()

    for idx, event in enumerate(dataloader):

        # readout generated by the misaligned detectors
        readout = event['readout_curr'].to(device)
        # Let us pretend for now that we know which three readouts are
        # generate from the same trajectory
        readout = readout.permute(0, 2, 1, 3).flatten(start_dim=-2, end_dim=-1)

        # we want to use misaligned readout to predict misaligned
        # parameter of the detectors. We load the starting and current
        # (misaligned) parameters of the detector and calculate
        # the target
        detector_curr = event['detector_curr'].to(device)
        detector_curr = detector_curr.flatten(start_dim=-2, end_dim=-1)

        detector_start = event['detector_start'].to(device)
        detector_start = detector_start.flatten(start_dim=-2, end_dim=-1)

        misalignment = detector_curr - detector_start

        # run the model, calculate loss, and update network
        pred_misal = model(readout)
        loss = loss_fn(pred_misal, misalignment)

        if optimizer is not None:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # bookkeeping
        # TODO: from the predicted misalignment, calculate the predicted
        # global coordinates of the intersection, from which we can get the
        # reconstructed particle parameter (vertices and directions).
        cumulator.update({'loss': loss.item(),
                          'mag': torch.pow(misalignment, 2).mean()})

        summary = cumulator.get_average()
        print(f'\n{desc} Step {idx}')
        pprint(summary, sort_dicts=False)

        torch.cuda.empty_cache()

    return summary

def get_parameters():
    """
    Get training configuration and training device
    """
    parser = argparse.ArgumentParser(description='')
    parser.add_argument('--config',
                        type    = str,
                        default = 'config.yaml',
                        help    = 'path to config file')
    parser.add_argument('--device',
                        type    = str,
                        default = 'cuda',
                        choices = ('cuda', 'cpu'),
                        help    = 'device to train the model on | default = cuda')
    parser.add_argument('--gpu-id',
                        type    = int,
                        default = 0,
                        help    = 'GPU to train the model on | default = 0')

    args = parser.parse_args()

    with open(args.config, 'r', encoding='UTF-8') as handle:
        config = yaml.safe_load(handle)

    return config, args.device, args.gpu_id


def train():
    """
    load config and train model
    """
    config, device, gpu_id = get_parameters()

    if device == 'cuda':
        torch.cuda.set_device(gpu_id)

    # checkpointing parameters
    checkpoint_path = config['checkpointing']['checkpoint_path']
    save_frequency  = config['checkpointing']['save_frequency']
    resume          = config['checkpointing']['resume']

    # training parameters
    num_epochs        = config['train']['num_epochs']
    num_warmup_epochs = config['train']['num_warmup_epochs']
    batch_size        = config['train']['batch_size']
    learning_rate     = config['train']['learning_rate']
    sched_steps       = config['train']['sched_steps']
    sched_gamma       = config['train']['sched_gamma']

    # Create model and resume if needed
    model = InducedPointFeaturizer(**config['model'])

    checkpointer = Checkpointer(model,
                                checkpoint_path = checkpoint_path,
                                save_frequency  = save_frequency)
    resume_epoch = 0
    if resume:
        resume_epoch = checkpointer.load()

    model = model.to(device)

    # loss function
    loss_fn = nn.MSELoss()

    # optimizer
    optimizer = AdamW(model.parameters(), lr = learning_rate)

    # schedular
    milestones = range(num_warmup_epochs, num_epochs, sched_steps)
    scheduler = MultiStepLR(optimizer,
                            milestones = milestones,
                            gamma      = sched_gamma)

    # data loader
    data_root = get_data_root()
    train_ds  = ROMDataset(data_root, split='train')
    valid_ds  = ROMDataset(data_root, split='test')
    train_ldr = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    valid_ldr = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)

    # training
    train_log = Path(checkpoint_path)/'train_log.csv'
    valid_log = Path(checkpoint_path)/'valid_log.csv'
    for epoch in range(resume_epoch + 1, num_epochs + 1):

        current_lr = get_lr(optimizer)
        print(f'current learning rate = {current_lr:.10f}')

        # train
        desc = f'Train Epoch {epoch} / {num_epochs}'
        train_stat = run_epoch(model,
                               loss_fn,
                               train_ldr,
                               desc      = desc,
                               optimizer = optimizer,
                               device    = device)

        # validation
        with torch.no_grad():
            desc = f'Validation Epoch {epoch} / {num_epochs}'
            valid_stat = run_epoch(model,
                                   loss_fn,
                                   valid_ldr,
                                   desc   = desc,
                                   device = device)

        # update learning rate
        scheduler.step()

        # save checkpoints
        checkpointer.save(epoch)

        # log the results
        for log, stat in zip([train_log, valid_log],
                             [train_stat, valid_stat]):

            stat.update({'lr': current_lr, 'epoch': epoch})
            df = pd.DataFrame(data=stat, index=[1])
            df.to_csv(log, index=False, float_format='%.6f',
                      mode   = 'a' if log.exists() else 'w',
                      header = not log.exists())


if __name__ == '__main__':
    train()
