"""
Train an detector calibration model on ROM data with IPOT
"""

import os
import argparse
from pathlib import Path
from tqdm import tqdm
import yaml
import numpy as np

# == torch ======================================
import torch
from torch.utils.data import DataLoader

# == sparse poi =================================
from rtal.datasets.dataset import ROMDataset
from rtal.utils import Checkpointer, Cumulator, get_data_root
from rtal.models.mlp import MLP
from rtal.geometry.line import (reconstruct,
                                calc_least_square_residual)


os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'


def to_numpy(tensor):
    """
    Squeeze the batch (if batch_size=1) and convert to numpy
    """
    return tensor.squeeze(0).detach().cpu().numpy()

def run_epoch(model,
              rounded_readout,
              loss_fn,
              dataloader, *,
              device,
              output_folder):
    """
    run one epoch on a data loader
    """

    # whether to use binned or continous readout
    readout_type = 'rounded' if rounded_readout else 'cont'

    cumulator = Cumulator()

    pbar = tqdm(enumerate(dataloader, start=1), total=len(dataloader), desc='evaluation')
    for event_idx, event in pbar:

        # readout generated by the misaligned detectors
        readout = event[f'readout_curr_{readout_type}'].to(device)
        # Let us pretend for now that we know which three readouts are
        # generate from the same trajectory
        # readout: (batch_size, num_detectors, num_particles, 2)
        #        ->(batch_size, num_particles, num_detectors, 2)
        readout = torch.transpose(readout, 1, 2)

        # get the correct and misaligned detectors
        detector_curr = event['detector_curr'].to(device)
        detector_start = event['detector_start'].to(device)

        # run the model, calculate loss, and update network
        detector_pred = model.inference(readout.flatten(-2, -1), randperm=False)

        detector_pred = detector_pred.reshape(-1, 3, 9)

        diff = loss_fn(detector_pred, detector_curr)

        # reconstruct the points and calculate the least square residual
        recon_pc = reconstruct(detector_pred, readout)
        residual_pc, closest_points_pc = calc_least_square_residual(
            recon_pc,
            particle_vertex       = event['particle_vertex'],
            particle_direction    = event['particle_direction'],
            averaged              = True,
            return_closest_points = True
        )

        # supervised loss
        loss = diff + 0 * residual_pc

        # metrics: line residual and differences to the
        # starting and current(target) detector parameters.
        # get line residual
        recon_cc = reconstruct(detector_curr, readout)
        residual_cc = calc_least_square_residual(
            recon_cc,
            particle_vertex    = event['particle_vertex'],
            particle_direction = event['particle_direction'],
            averaged           = True
        )

        recon_sc = reconstruct(detector_start, readout)
        residual_sc = calc_least_square_residual(
            recon_sc,
            particle_vertex    = event['particle_vertex'],
            particle_direction = event['particle_direction'],
            averaged           = True
        )

        # get differences to the start and current (target) detector parameters.
        diff_to_start = loss_fn(detector_pred, detector_start)
        epsilon       = loss_fn(detector_curr, detector_start)

        # bookkeeping
        # TODO: from the predicted misalignment, calculate the predicted
        # global coordinates of the intersection, from which we can get the
        # reconstructed particle parameter (vertices and directions).
        cumulator.update({'loss'    : loss.item(),
                          'res_pc'  : residual_pc.item(),
                          'diff_pc' : diff.item(),
                          # metrics:
                          'diff_ps' : diff_to_start.item(),
                          'res_cc'  : residual_cc.item(),
                          'res_sc'  : residual_sc.item(),
                          'epsilon' : epsilon.item()})

        summary = cumulator.get_average()
        pbar.set_postfix(summary)

        # we need to save recon_pc, closest_points, and recon_cc
        results = {'detector_curr'      : to_numpy(detector_curr),
                   'detector_start'     : to_numpy(detector_start),
                   'particle_vertex'    : to_numpy(event['particle_vertex']),
                   'particle_direction' : to_numpy(event['particle_direction']),
                   'recon_cc'           : to_numpy(recon_cc),
                   'recon_pc'           : to_numpy(recon_pc),
                   'closest_points'     : to_numpy(closest_points_pc)}

        np.savez_compressed(output_folder/f'sample_{event_idx}.npz', **results)

    return summary

def get_parameters():
    """
    Get training configuration and training device
    """
    parser = argparse.ArgumentParser(description='')
    parser.add_argument('--config',
                        type    = str,
                        default = 'config.yaml',
                        help    = 'path to config file | config.yaml')
    parser.add_argument('--device',
                        type    = str,
                        default = 'cuda',
                        choices = ('cuda', 'cpu'),
                        help    = 'device to train the model on | default = cuda')
    parser.add_argument('--gpu-id',
                        type    = int,
                        default = 0,
                        help    = 'GPU to train the model on | default = 0')
    parser.add_argument('--batch-size',
                        type    = int,
                        default = None,
                        help    = ('batch size | default = None, '
                                   'same as batch_size in config'))
    parser.add_argument('--num-eval-particles',
                        type    = int,
                        default = None,
                        help    = ('number of particles used for evaluation | '
                                   'default = None (same as train)'))
    parser.add_argument('--output-folder',
                        type    = str,
                        default = None,
                        help    = ("output folder | default = None, "
                                   "don't save results"))

    args = parser.parse_args()

    with open(args.config, 'r', encoding='UTF-8') as handle:
        config = yaml.safe_load(handle)

    return (config,
            args.device,
            args.gpu_id,
            args.batch_size,
            args.num_eval_particles,
            args.output_folder)


def evaluate():
    """
    load config and evaluate a model
    """
    config, device, gpu_id, batch_size, num_eval_particles, output_folder = get_parameters()

    if output_folder is not None:
        output_folder = Path(output_folder)
        output_folder.mkdir(parents=True, exist_ok=True)


    if device == 'cuda':
        torch.cuda.set_device(gpu_id)

    # checkpointing parameters
    checkpoint_path = config['checkpointing']['checkpoint_path']

    # training parameters
    if batch_size is None:
        batch_size = config['train']['batch_size']

    # set up checkpoint folder
    checkpoint_path = Path(checkpoint_path)

    # Create model and resume if needed
    model = MLP(**config['model'])

    # Checkpointer
    checkpointer = Checkpointer(model, checkpoint_path=checkpoint_path)
    checkpointer.load(device=device)
    model = model.to(device)

    # loss function
    loss_fn = torch.nn.MSELoss()

    # data loader
    data_root = get_data_root()

    if num_eval_particles is None:
        num_particles = config['data']['num_particles']
    else:
        num_particles = num_eval_particles

    rounded_readout = config['data']['rounded']

    valid_ds  = ROMDataset(data_root, split='test', num_particles=num_particles)
    valid_ldr = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)

    # evaluation
    with torch.no_grad():
        run_epoch(model,
                  rounded_readout,
                  loss_fn,
                  valid_ldr,
                  device        = device,
                  output_folder = output_folder)


if __name__ == '__main__':
    evaluate()
